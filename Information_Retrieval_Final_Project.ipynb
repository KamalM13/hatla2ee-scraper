{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR+36KjDwTJpIquR6TFvOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamalM13/hatla2ee-scraper/blob/main/Information_Retrieval_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping used cars listed on sale from all brands on The Egyptian Website Hatla2ee\n",
        "We will make use of 2 basic libraries and BeautifulSoup\n",
        "\n",
        "*   Pandas\n",
        "*   Requests\n",
        "*   BeautifulSoup\n",
        "\n",
        "# Problem Statement\n",
        "## Phase 1: Data Scraping and Collection\n",
        "### Target Website: [Hatla2ee](https://eg.hatla2ee.com/en/)\n",
        "![picture](https://i.ibb.co/MMdJspN/image.png)\n",
        "\n",
        "### Data to be scraped is as follows\n",
        "* Name\n",
        "* Brand\n",
        "* Model\n",
        "* Color\n",
        "* Manafacture Year\n",
        "* Price\n",
        "* Mileage\n",
        "* Location\n",
        "* Listing Date\n",
        "\n",
        "### Mission\n",
        "This is a two phase project where the 1st phase will prepare a dataset for data modeling in PowerBI. In this notebook, we will be scraping all car listings for each brand on the Egyptian website, Hatla2ee.\n",
        "\n",
        "### Methodolgy\n",
        "* Download the Hatla2ee search page and convert it's content to a BeautifulSoup object\n",
        "* Derive an algorithm to navigate all search pages according to a car brand\n",
        "* Parse the required information through multiple processing steps and write the data to a .csv file\n",
        "* Preview data using Pandas\n"
      ],
      "metadata": {
        "id": "MrE9TSxM0nCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Hatla2ee Page\n",
        "This section will scrap all car listings for each brand and it's subsequent pages\n",
        "\n",
        "**The Car brands are as follows:**\n"
      ],
      "metadata": {
        "id": "R07W2zn63YeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_dict = {\n",
        "    \"Abarth\": 255,\n",
        "    \"Acura\": 247,\n",
        "    \"Alfa Romeo\": 48,\n",
        "    \"Aston Martin\": 106,\n",
        "    \"Audi\": 23,\n",
        "    \"Baic\": 110,\n",
        "    \"Bentley\": 104,\n",
        "    \"Bestune\": 221,\n",
        "    \"BMW\": 8,\n",
        "    \"Borgward\": 278,\n",
        "    \"Brilliance\": 5,\n",
        "    \"Bugatti\": 138,\n",
        "    \"Buick\": 70,\n",
        "    \"Byd\": 6,\n",
        "    \"Cadillac\": 51,\n",
        "    \"Canghe\": 128,\n",
        "    \"Chana\": 132,\n",
        "    \"Changan\": 77,\n",
        "    \"Chery\": 94,\n",
        "    \"Chevrolet\": 1,\n",
        "    \"Chrysler\": 53,\n",
        "    \"CitroÃ«n\": 9,\n",
        "    \"Cupra\": 233,\n",
        "    \"Daewoo\": 14,\n",
        "    \"Daihatsu\": 27,\n",
        "    \"Datsun\": 84,\n",
        "    \"DFSK\": 126,\n",
        "    \"Dodge\": 54,\n",
        "    \"Domy\": 158,\n",
        "    \"Dongfeng\": 114,\n",
        "    \"Dorcen\": 279,\n",
        "    \"Ds\": 159,\n",
        "    \"Emgrand\": 72,\n",
        "    \"Exeed\": 237,\n",
        "    \"Faw\": 65,\n",
        "    \"Ferrari\": 90,\n",
        "    \"Fiat\": 4,\n",
        "    \"Ford\": 45,\n",
        "    \"Forthing\": 234,\n",
        "    \"Foton\": 102,\n",
        "    \"GAC\": 108,\n",
        "    \"Gaz\": 96,\n",
        "    \"Geely\": 59,\n",
        "    \"Genesis\": 231,\n",
        "    \"Gmc\": 57,\n",
        "    \"Great Wall\": 64,\n",
        "    \"Hafei\": 63,\n",
        "    \"Haima\": 120,\n",
        "    \"Hanteng\": 162,\n",
        "    \"Haval\": 146,\n",
        "    \"Hawtai\": 136,\n",
        "    \"Honda\": 41,\n",
        "    \"Hongqi\": 212,\n",
        "    \"Hummer\": 50,\n",
        "    \"Hyundai\": 12,\n",
        "    \"Ineos\": 285,\n",
        "    \"Infiniti\": 60,\n",
        "    \"Isuzu\": 61,\n",
        "    \"Jac\": 62,\n",
        "    \"Jaguar\": 28,\n",
        "    \"Jeep\": 29,\n",
        "    \"Jetour\": 163,\n",
        "    \"Jonway\": 68,\n",
        "    \"Kaiyi\": 227,\n",
        "    \"Karry\": 100,\n",
        "    \"Kenbo\": 122,\n",
        "    \"Keyton\": 124,\n",
        "    \"KGM (ssangyong)\": 287,\n",
        "    \"Kia\": 3,\n",
        "    \"Lada\": 2,\n",
        "    \"Lamborghini\": 116,\n",
        "    \"Lancia\": 75,\n",
        "    \"Land Rover\": 56,\n",
        "    \"Landwind\": 142,\n",
        "    \"Leapmotor\": 253,\n",
        "    \"Lexus\": 52,\n",
        "    \"Lifan\": 69,\n",
        "    \"Lincoln\": 46,\n",
        "    \"Lotus\": 248,\n",
        "    \"Lynkco\": 252,\n",
        "    \"Mahindra\": 82,\n",
        "    \"Maserati\": 92,\n",
        "    \"Maxus\": 228,\n",
        "    \"Mazda\": 16,\n",
        "    \"McLaren\": 243,\n",
        "    \"Mercedes\": 40,\n",
        "    \"Mercury\": 76,\n",
        "    \"MG\": 71,\n",
        "    \"Mini\": 66,\n",
        "    \"Mitsubishi\": 13,\n",
        "    \"Nissan\": 21,\n",
        "    \"Opel\": 22,\n",
        "    \"Perodua\": 240,\n",
        "    \"Peugeot\": 10,\n",
        "    \"Polestar\": 275,\n",
        "    \"Pontiac\": 67,\n",
        "    \"Porsche\": 73,\n",
        "    \"Proton\": 44,\n",
        "    \"Renault\": 30,\n",
        "    \"Rolls Royce\": 224,\n",
        "    \"Saab\": 74,\n",
        "    \"Saipa\": 118,\n",
        "    \"Scion\": 86,\n",
        "    \"Seat\": 47,\n",
        "    \"Senova\": 98,\n",
        "    \"Skoda\": 43,\n",
        "    \"Skywell\": 284,\n",
        "    \"Smart\": 130,\n",
        "    \"Sokon\": 140,\n",
        "    \"Soueast\": 88,\n",
        "    \"Speranza\": 7,\n",
        "    \"Ssang Yong\": 49,\n",
        "    \"Subaru\": 39,\n",
        "    \"Suzuki\": 31,\n",
        "    \"Tank\": 249,\n",
        "    \"Tata\": 80,\n",
        "    \"Tesla\": 134,\n",
        "    \"Toyota\": 36,\n",
        "    \"Vgv\": 281,\n",
        "    \"Volkswagen\": 35,\n",
        "    \"Volvo\": 33,\n",
        "    \"Zeekr\": 273,\n",
        "    \"ZNA\": 112,\n",
        "    \"Zotye\": 78\n",
        "}\n"
      ],
      "metadata": {
        "id": "WH9-vOtADR5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping Section\n",
        "This section is concerned with scraping all HTML pages for each brand and their subsequent search pages.\n",
        "Our approach is farily simple, we will do 2 requests per page.\n",
        "1st request is to acquire the number of search pages\n",
        "per brand and the >2 requests are saving all the pages\n",
        "as a beautiful soup object."
      ],
      "metadata": {
        "id": "G43JUk8wA2h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Uk4zkAs70h8o"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "#The headers change the request from being generated by python to a request similar to an actual user through a browser\n",
        "request_headers = {\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "# Since hatla2ee supports pagination, we have to implement a function to get the nth page for every brand\n",
        "def get_brand_pages_number(url):\n",
        "  response = requests.get(url,headers = request_headers)\n",
        "  if not response.ok:\n",
        "    print(\"Error:\",response.status_code)\n",
        "    raise Exception(\"Failed to fetch\" +  url)\n",
        "  doc = bs(response.text)\n",
        "  #Finding the number of search pages per brand\n",
        "  number = doc.find_all(\"a\",class_ = \"paginate\")\n",
        "  if len(number) != 0:\n",
        "    return len(number)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_brand_for_nth_page(brand):\n",
        "  #Example url: https://eg.hatla2ee.com/en/car/search?make=8&city=0&dateMin=0&priceMin=&model=&body=&dateMax=0&priceMax=&page=2\n",
        "  url_start = 'https://eg.hatla2ee.com/en/car/search?make='\n",
        "  # Second part represents subsequent queries, we will not be adjusting those\n",
        "  url_end = '&city=0&dateMin=0&priceMin=&model=&body=&dateMax=0&priceMax=&page='\n",
        "  #changing the pages according to the brand\n",
        "  brand_url = url_start + str(brand) + url_end\n",
        "  #Getting the number of pages per brand\n",
        "  pages = get_brand_pages_number(brand_url)\n",
        "\n",
        "  #making sure that we iterate over brands with only 1 search page\n",
        "  if pages == 0:\n",
        "    pages += 1\n",
        "  docs = []\n",
        "  for i in range(1,pages+1):\n",
        "    #the concatenated link (link + page number)\n",
        "    temp = brand_url + str(i)\n",
        "    response = requests.get(temp)\n",
        "    doc = bs(response.text)\n",
        "    if not response.ok:\n",
        "        print('Status Code:', response.status_code)\n",
        "        raise Exception('Failed to get web page' + brand_url)\n",
        "    docs.append(doc)\n",
        "\n",
        "  return docs\n",
        "\n",
        "store = []\n",
        "for car in car_dict:\n",
        "  #main line to call the function for every brand\n",
        "  docs = get_brand_for_nth_page(car_dict[car])\n",
        "  store.append(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store[0]"
      ],
      "metadata": {
        "id": "HH3nye1yYvai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping the required values\n",
        "Now we have all the website pages as BS object, going through\n",
        "the \"css\" classes, we can identify the required fields for our\n",
        "dataset. Again, the approach is pretty simple, we initilaize an iterator and a find function for every value and loop over them for every page in our collection of brand pages."
      ],
      "metadata": {
        "id": "BzEiROflFbGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "car_info_list = []\n",
        "store_counter = 0\n",
        "for i in range(0,len(store)):\n",
        "  for docs in store[store_counter]:\n",
        "    #an iterator for every target value\n",
        "    color_and_mileage = docs.find_all(\"span\", class_=\"newCarListUnit_metaTag\")\n",
        "    color_and_mileage_counter = 0\n",
        "    location = docs.find_all(\"a\", href=re.compile(\"/en/car/city\"))\n",
        "    location_counter = 0\n",
        "    date = docs.find_all(\"div\", class_ = \"otherData_Date\")\n",
        "    date_counter = 0\n",
        "    price = docs.find_all(\"div\", class_= \"main_price\")\n",
        "    price_counter = 0\n",
        "    for header in docs.find_all(\"div\", class_=\"newCarListUnit_header\"):\n",
        "        car_info = {}\n",
        "        anchor_tag = header.find(\"a\")\n",
        "        if anchor_tag:\n",
        "            car_info[\"Name\"] = anchor_tag.text.strip()\n",
        "            car_info[\"Brand\"] = car_info[\"Name\"].split()[0]\n",
        "            car_info[\"Model\"] = ' '.join(car_info[\"Name\"].split()[1:-1])\n",
        "            car_info[\"Manafacture Year\"] = car_info[\"Name\"].split()[-1]\n",
        "\n",
        "            car_info[\"Color\"] = color_and_mileage[color_and_mileage_counter].text.strip()\n",
        "            car_info[\"Mileage\"] = color_and_mileage[color_and_mileage_counter+1].text.strip()\n",
        "            color_and_mileage_counter+=2\n",
        "\n",
        "            price_tag = price[price_counter].find('a')\n",
        "            car_info[\"Price\"] = price_tag.text.strip()\n",
        "            price_counter += 1\n",
        "\n",
        "            car_info[\"Location\"] = location[location_counter].text.strip()\n",
        "            location_counter += 1\n",
        "\n",
        "            date_tag = date[date_counter].find(\"span\")\n",
        "            car_info[\"Listing Date\"] = date_tag.text.strip()\n",
        "            date_counter += 1\n",
        "\n",
        "            car_info_list.append(car_info)\n",
        "  store_counter += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8VF5AiMp4adR"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(car_info_list)\n",
        "df['Mileage'] = df['Mileage'].str.replace(r'[\\s,]', '', regex=True).str.replace(r'Km', '0', regex=True).astype(int)\n",
        "df['Price'] = df['Price'].str.replace(\"-\",\"0\")\n",
        "df['Price'] = df['Price'].str.replace(r'[^\\d-]', '', regex=True).replace('', '0', regex=True).astype(int)\n",
        "\n",
        "df.head()\n",
        "df.to_csv('hatla2ee_scrap_data.csv')"
      ],
      "metadata": {
        "id": "PMJbkjgjQnu-"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97AzMVYLR1q6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}